<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ByteTransformer源码解读 | Invisible Cities </title> <meta name="author" content="Damons "> <meta name="description" content="记录学习ByteTransformer过程中的代码释疑"> <meta name="keywords" content="jekyll, al-folio, damons, valdrada, joney"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" href="/assets/css/tufte-note.css?51b2b62d214d64cc821d90a4e134513f"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img//icon/damons.ico?2d9b67e87d277ec91f5abe5b48e8eef8"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://damonsj.github.io/AI/2023-09-14-bytetransformer.html"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Invisible Cities </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/math/">Math </a> </li> <li class="nav-item "> <a class="nav-link" href="/programming/">Programming </a> </li> <li class="nav-item active"> <a class="nav-link" href="/AI/">AI <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/reading/">Reading </a> </li> <li class="nav-item "> <a class="nav-link" href="/life/">Life </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">ByteTransformer源码解读</h1> <p class="post-meta"> Created on September 14, 2023 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2023   ·   <i class="fa-solid fa-hashtag fa-sm"></i> 源码解读   ·   <i class="fa-solid fa-tag fa-sm"></i> AI </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="一-简介">一、 简介</h1> <p>ByteTransformer库是字节开源的针对Bert模型的推理库，其中没有读入模型的部分，只有完整的执行逻辑。花了一些时间看懂了ByteTransformer中的非cutlass部分的源码，记录一下整个<a href="https://github.com/bytedance/ByteTransformer" rel="external nofollow noopener" target="_blank">ByteTransformer</a>的整个执行流程。建议先读一遍<a href="https://arxiv.org/abs/2210.03052" rel="external nofollow noopener" target="_blank">ByteTransformer论文</a> 会对理解代码有很大的帮助。</p> <h1 id="二-bert模型的结构">二、 Bert模型的结构</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/bert-480.webp 480w,/assets/img/AI/bytetransformer/bert-800.webp 800w,/assets/img/AI/bytetransformer/bert-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/bert.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>上图是Bert整个模型的结构，对于B个batch的输入，假设max seq len是S，那么B$\times$S个输入词，经过embedding之后，得到了一个$B\times S \times H$个矩阵，其中$H = head_num \times size_per_head$是Hidden Dim。这个输入矩阵首先经过一个线性层会得到QKV三个矩阵，然后就是进入self-attention块，之后经过残差层和layernorm层，然后进入FFN块， 之后再经过残差层和layer norm得到结果，得到的结果矩阵和输入是同样的大小，都是$B\times S \times H$的矩阵。关于从原始输入句子输入如何经过embedding层得到输入数据，参考下图（假设H=768）：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/embedding-480.webp 480w,/assets/img/AI/bytetransformer/embedding-800.webp 800w,/assets/img/AI/bytetransformer/embedding-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/embedding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> embedding </div> <h1 id="三bytetransformer的执行逻辑及代码分析">三、ByteTransformer的执行逻辑及代码分析</h1> <h2 id="1-bytetransformer的代码整体流程">1. ByteTransformer的代码整体流程</h2> <p>如图:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/bytetransformer.process-480.webp 480w,/assets/img/AI/bytetransformer/bytetransformer.process-800.webp 800w,/assets/img/AI/bytetransformer/bytetransformer.process-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/bytetransformer.process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>ByteTransformer代码的整体执行流程还是比较清晰的，主流程函数是<code class="language-plaintext highlighter-rouge">bert_infer()</code>函数，这个函数包含了整个bert推理的各个大的流程，主要分成8个主要的步骤，这几个步骤可以对应在下图中，具体可以参考<a href="https://arxiv.org/abs/2210.03052" rel="external nofollow noopener" target="_blank">ByteTransformer论文</a>。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/bert_architecture-480.webp 480w,/assets/img/AI/bytetransformer/bert_architecture-800.webp 800w,/assets/img/AI/bytetransformer/bert_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/bert_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>step1. 如果设置了<code class="language-plaintext highlighter-rouge">remove_padding</code>标志,处理输入，去掉padding的操作。（<code class="language-plaintext highlighter-rouge">if (is_remove_padding_) {...}</code>）</li> <li>step2. 线性层。 得到QKV矩阵，也就是第一个GEMM操作。（<code class="language-plaintext highlighter-rouge">dense_layer_kernel_launcher</code>）</li> <li>step3. attention层。这个地方计算整个attention块。（<code class="language-plaintext highlighter-rouge">attention_layer_-&gt;infer(attention_infer_param)</code>）</li> <li>step4.线性层。attention之后的线性层。（<code class="language-plaintext highlighter-rouge">dense_layer_kernel_launcher</code>）</li> <li>step5.残差和layernorm层。（<code class="language-plaintext highlighter-rouge">add_bias_input_layernorm_kernel_launcher</code>）</li> <li>step6.FFN中的升维层和激活层，融合了GEMM和激活层。（<code class="language-plaintext highlighter-rouge">gemm_bias_gelu</code>）</li> <li>step7.FFN中的降维层，也是个GEMM。（<code class="language-plaintext highlighter-rouge">dense_layer_kernel_launcher</code>）</li> <li>step8.残差和layernorm层。（<code class="language-plaintext highlighter-rouge">if (is_remove_padding_) {...}</code>）</li> </ul> <h2 id="2-关于remove-padding的解释step-1">2. 关于remove padding的解释（step 1）</h2> <h3 id="1-为什么会有padding">1. 为什么会有padding？</h3> <p>​ 对于输入，我们每次给模型的数据有个最大的长度也就是max seq len，也就是输入的一句话中最多有多少个单词，但是每次实际的输入并不是都是最长的，不够的怎么办？一般就会pad成最长，这就是padding的由来。 假设max seq len设置成了5，我们输入了一句话也就是一个batch是： I love you 。那么经过分词每个词都会表达成一个H(hidden dim)维的向量，那么这句话就会表示成3$\times$H个向量（假设是简单的空格分词），因为max seq len是5，也就是需要5$\times$H个向量，我们现在才有3个，剩下的2个需要pad，也就是最终我们会有5$\times$H个向量，最后的2个是pad的向量，一样参与计算但是不会有用就是了。下图展示一个例子，其中Batch是3,根据实际的输入每个batch pad的数据多少都不一样。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/padding-480.webp 480w,/assets/img/AI/bytetransformer/padding-800.webp 800w,/assets/img/AI/bytetransformer/padding-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/padding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="2-removing-padding">2. removing padding</h3> <p>​ 需要注意的是ByteTransformer输入的矩阵默认就是经过padding之后的，也就是$B \times S \times H$的矩阵。显然padding之后的数据是没有什么用的，但是一样参与了计算，为了减少计算量需要remove padding。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/padding-process-480.webp 480w,/assets/img/AI/bytetransformer/padding-process-800.webp 800w,/assets/img/AI/bytetransformer/padding-process-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/padding-process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>​ 怎么样才能remove padding？我们只需要记录目标矩阵和原矩阵之间的行的对应关系就行了，我们需要记录每个batch的实际的seq的长度，以及每个batch中的序列标号的偏移量就行了（也就是prefix sum），参考下图可以进一步理解：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/remove-480.webp 480w,/assets/img/AI/bytetransformer/remove-800.webp 800w,/assets/img/AI/bytetransformer/remove-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/remove.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>对于给出的例子，有3个batch，也就是B=3，max seq len是5 也就是S=5，第一个batch的有效长度是3，第二个有效长度是2，第三个有效长度是4，在ByteTransformer中分别记录了途中batch seq offset 和 wordidx这两个数据，通过batch seq offset 可以得到每个batch的实际的有效长度，通过wordidx可以知道目标的矩阵的行和原始输入矩阵的行的对应关系。计算这个数据的kernel是<code class="language-plaintext highlighter-rouge">build_sequence_length_padding_offset_kernelLauncher</code>,我们来看一下具体的实现。这里需要有个前提知识是ByteTransformer计算有效行数，是通过attention mask来计算的。attention mask的大小是$B \times S \times S$，其中每个batch的维度都是$S \times S$，这个矩阵是个mask矩阵，矩阵中的元素是0或者1，值代表当前word对其他word的可见性。如果输入的某一行是pad的也就是无效的，那么对应在mask 矩阵中那一行的数值应该都是0，因为这一行对其他行都是无效的。第一行对其他的有效行总是可见的，所以第一行的非0的值的个数就是这个batch有效的行数。所以ByteTransformer统计每个batch的有效行数是通过计算attention mask的第一行的非0值的个数来计算的，下图的mask只是一个示例：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/mask-480.webp 480w,/assets/img/AI/bytetransformer/mask-800.webp 800w,/assets/img/AI/bytetransformer/mask-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/mask.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>具体的代码为：</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="nf">build_sequence_length_padding_offset_kernelLauncher</span><span class="p">(</span><span class="k">const</span> <span class="n">T</span> <span class="o">*</span><span class="n">atten_mask</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">batch_idx</span><span class="p">,</span>
                                                         <span class="kt">int</span> <span class="o">*</span><span class="n">word_idx</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">valid_word_num</span><span class="p">,</span>
                                                         <span class="k">const</span> <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span>
                                                         <span class="k">const</span> <span class="kt">int</span> <span class="n">max_seq_len</span><span class="p">,</span>
                                                         <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// 这里抛线程的逻辑是1个warp计算1个batch的有效行数</span>
  <span class="c1">// 因为线程数有1024的限制，如果batch_size * 32超过了1024，那么1个warp可能就处理多个batch</span>
  <span class="c1">// 所以总共需要batch个warp，1个warp总共32个线程，所以线程数就是batch*32</span>
  <span class="n">dim3</span> <span class="n">block</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">32</span><span class="p">);</span>  <span class="c1">// one warp per sequence</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">block</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">)</span>
    <span class="n">block</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">;</span>

  <span class="c1">//这里的shared memory的大小是(2 * batch_size + 1)个int</span>
  <span class="c1">//是因为存储offset 需要 batch+1个（prefix sum）， 中间过程存储有效行数是batch个</span>
  <span class="n">parallel_prefix</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
      <span class="n">atten_mask</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">);</span>
  <span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">valid_word_num</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">,</span>
                  <span class="n">stream</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/prefixsum-480.webp 480w,/assets/img/AI/bytetransformer/prefixsum-800.webp 800w,/assets/img/AI/bytetransformer/prefixsum-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/prefixsum.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="p">&gt;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">parallel_prefix</span><span class="p">(</span><span class="k">const</span> <span class="n">T</span> <span class="o">*</span><span class="n">atten_mask</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">batch_idx</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">word_idx</span><span class="p">,</span>
                                <span class="k">const</span> <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">max_seq_len</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_count</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">5</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">warp_id</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">&gt;&gt;</span> <span class="mi">5</span><span class="p">;</span> <span class="c1">// 这个是warp的id</span>
  <span class="kt">int</span> <span class="n">warp_tid</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">&amp;</span> <span class="mh">0x1F</span><span class="p">;</span><span class="c1">//这个是线程的id 这个类似对32求余操作</span>

  <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">base</span><span class="p">[];</span>

  <span class="kt">int</span> <span class="o">*</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">base</span><span class="p">;</span>
  <span class="kt">int</span> <span class="o">*</span><span class="n">seq_offset</span> <span class="o">=</span> <span class="n">base</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">;</span>
  <span class="c1">// 因为线程数有1024的限制，如果batch_size * 32超过了1024，那么1个warp可能就处理多个batch</span>
  <span class="c1">// 所以需要加上一个循环处理一下这种情况</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">wid</span> <span class="o">=</span> <span class="n">warp_id</span><span class="p">;</span> <span class="n">wid</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">;</span> <span class="n">wid</span> <span class="o">+=</span> <span class="n">warp_count</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">//同理这个地方是1个warp内的操作，1个warp总共32个线程，1个线程处理1行中的1个元素</span>
    <span class="c1">//但是如果这行超过了32个元素，那么1个线程就多处理几个就可以了</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">warp_tid</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">max_seq_len</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">/</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">32</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">T</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_seq_len</span> <span class="o">?</span> <span class="n">atten_mask</span><span class="p">[</span><span class="n">wid</span> <span class="o">*</span> <span class="n">max_seq_len</span> <span class="o">*</span> <span class="n">max_seq_len</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
      <span class="c1">// unsigned __ballot_sync(unsigned mask, int predicate);</span>
      <span class="c1">// 意思是返回一个 32 位无符号整数，代表了该warp内变量 predicate 的非零值分布情况</span>
      <span class="c1">//（即线程 predicate 为零的该函数返回值该位为 0，线程 predicate 非零的该函数返回值该位为 1 ）</span>
      <span class="c1">// 也就是返回一个32位整数 其中如果线程N的predicate是true，那么返回值第N位是1，否则返回值第N位是0</span>
      <span class="c1">// __popc这个原语的意思是统计二进制位中1的个数</span>
      <span class="c1">// __ballot_sync这个操作是同步的，也就是统计这个warp中32个线程的mask &gt;= (T)0.5f的个数</span>
      <span class="c1">//需要注意的是 每次for循环__ballot_sync这个就会重新统计,也就是假设warp_tid=0的时候__ballot_sync统计的是</span>
      <span class="c1">//0~31个元素中的mask &gt;= (T)0.5f的个数，warp_tid=32的时候统计的是32~63个元素中的mask &gt;= (T)0.5f的个数</span>
      <span class="c1">//count是寄存器的值，每个线程都有个count</span>
      <span class="n">count</span> <span class="o">+=</span> <span class="n">__popc</span><span class="p">(</span><span class="n">__ballot_sync</span><span class="p">(</span><span class="mh">0xFFFFFFFF</span><span class="p">,</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">T</span><span class="p">)</span><span class="mf">0.5</span><span class="n">f</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="c1">//只在线程0写出就可以了 不用重复写</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">warp_tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">seq_len</span><span class="p">[</span><span class="n">wid</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">// 这个地方需要同步，意思是把所有warp计算完成，也就是每个batch的有效word数就统计完成了</span>
  <span class="n">__syncthreads</span><span class="p">();</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">warp_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 这个是通过seq_len计算prefix sum</span>
    <span class="c1">// 假设 seq_len 有3个值 分别是 2，4，5</span>
    <span class="c1">// 那么seq_offset分别是0,2,6,11</span>
    <span class="c1">// 因为只需要对seq_len计算，所以只需要1个warp就可以了</span>
    <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">temp</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">warp_tid</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">((</span><span class="n">batch_size</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">/</span> <span class="mi">32</span><span class="p">)</span> <span class="o">*</span> <span class="mi">32</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">32</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">offset</span> <span class="o">=</span> <span class="n">warp_tid</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">?</span> <span class="n">temp</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
      <span class="kt">int</span> <span class="n">len</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="o">?</span> <span class="n">seq_len</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
      <span class="c1">// 这个是个warp sum操作，需要注意的是，warpPrefixSum这个函数中只统计线程id比warp_tid小的数的和</span>
      <span class="c1">// 也就是warp_tid如果是2 那么只累加0 和 1的值就可以了,就是prefix sum</span>
      <span class="n">temp</span> <span class="o">=</span> <span class="n">warpPrefixSum</span><span class="p">(</span><span class="n">warp_tid</span><span class="p">,</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">len</span><span class="p">);</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">seq_offset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span> <span class="o">-</span> <span class="n">len</span><span class="p">;</span>
      <span class="c1">// T __shfl_sync(unsigned mask, T var, int srcLane, int width=warpSize);</span>
      <span class="c1">// 表示被 mask 指定的线程返回标号为 srcLane 的线程中的变量 var 的值，其余线程返回0 。类似 broadcast，mask 是参与的线程掩码，如0xffffffff</span>
      <span class="c1">// var 是待广播的值，srclane 是被广播的 laneid，warpsize 是参与 warp 大小；</span>
      <span class="c1">// 计算完前32个值之后，需要将prefix sum的结果广播到其他的线程，第二次循环的时候会用到这个temp的值</span>
      <span class="n">temp</span> <span class="o">=</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">temp</span><span class="p">,</span> <span class="mi">31</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">//最后的temp就是总的有效word数</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">warp_tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">seq_offset</span><span class="p">[</span><span class="n">batch_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">__syncthreads</span><span class="p">();</span>

  <span class="c1">// 这个就是把seq len offset 写出</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">tid</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">batch_size</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>
    <span class="n">batch_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">seq_offset</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="c1">// 这个就是把原始矩阵和去掉pad之后的矩阵的seq的id的对应关系 写出</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">wid</span> <span class="o">=</span> <span class="n">warp_id</span><span class="p">;</span> <span class="n">wid</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">;</span> <span class="n">wid</span> <span class="o">+=</span> <span class="n">warp_count</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">seq_offset</span><span class="p">[</span><span class="n">wid</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">warp_tid</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">seq_len</span><span class="p">[</span><span class="n">wid</span><span class="p">];</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">32</span><span class="p">)</span>
      <span class="n">word_idx</span><span class="p">[</span><span class="n">offset</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">wid</span> <span class="o">*</span> <span class="n">max_seq_len</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>之后的<code class="language-plaintext highlighter-rouge">compressBertInput_kernelLauncher</code>kernel就简单了，就是把pad的数据给去掉，需要注意的是这个地方其实就是个数据的拷贝，一次写4个float也就是8个half，所以抛线程的时候是H’/4,需要注意这个H’是hidden_dim/2。</p> <h2 id="3-关于第一个线性层的解释step-2">3. 关于第一个线性层的解释（step 2）</h2> <p>第一个线性层是计算$Input \times QKV_{kernel}$，代码中其实是调用了cublas的gemm，也就是调用了函数<a href="https://docs.nvidia.com/cuda/archive/10.1/cublas/index.html#cublas-GemmEx" rel="external nofollow noopener" target="_blank">cublasGemmEx</a>，在输入给函数参数中其实是计算了$QKV_{kernel} \times Input$，为什么呢？下图是解释，这里需要注意的是cublas的矩阵是<strong>列主序</strong>的，其实不需要这么算，只要记住内存一定，列主序就是行主序的转置就行了。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/mat_mul-480.webp 480w,/assets/img/AI/bytetransformer/mat_mul-800.webp 800w,/assets/img/AI/bytetransformer/mat_mul-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/mat_mul.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="4关于attention层的解释step-3">4.关于Attention层的解释（step 3）</h2> <p>attention层是主要的代码部分，也是主要的优化的代码，就像之前说的，attention层有两个主要的优化方法，一部分是cutlass的部分，这部分暂时不看，另一部分就是自己写的kernel，叫做FMA(fused multi attention)，attention层的整体逻辑是:</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">virtual</span> <span class="kt">void</span> <span class="nf">infer</span><span class="p">(</span><span class="n">AttentionInferParam</span> <span class="n">infer_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">use_fused_attention_</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">infer_param</span><span class="p">.</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">80</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">is_remove_padding_</span><span class="p">)</span>
          <span class="n">fused_rm_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
        <span class="k">else</span>
          <span class="n">fused_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">is_remove_padding_</span><span class="p">)</span>
          <span class="n">fused_long_rm_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
        <span class="k">else</span>
          <span class="n">fused_long_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span>
      <span class="nf">nofused_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
  <span class="p">}</span>

</code></pre></div></div> <p>这里分成两个部分，一个是fused的kernel，一个是no fused的kernel， fused的就是做了优化的，将attention的几个计算步骤融合到了一个kernel，no fused就是一步步计算。我们先从no fused的开始以便熟悉计算流程。</p> <h3 id="1-no-fused-kernel源码分析">1. no fused kernel源码分析</h3> <p>这部分的代码主要的计算步骤如下：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/nofused-480.webp 480w,/assets/img/AI/bytetransformer/nofused-800.webp 800w,/assets/img/AI/bytetransformer/nofused-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/nofused.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="0add-bias">0.add bias</h4> <p>​ 这一步的kernel是<code class="language-plaintext highlighter-rouge">add_QKV_bias</code>或者<code class="language-plaintext highlighter-rouge">add_QKV_bias_padding</code>， 主要的目的有两个： 将bias加上和矩阵数据的转置，如果是<code class="language-plaintext highlighter-rouge">is_remove_padding_</code>为true，那么<code class="language-plaintext highlighter-rouge">add_QKV_bias_padding</code>还会把之前去掉的padding的内容还原回来。</p> <p>​ 这一步的输入是上一步经过GEMM之后的QKV矩阵，输入的矩阵的数据排布如下：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/qkv-480.webp 480w,/assets/img/AI/bytetransformer/qkv-800.webp 800w,/assets/img/AI/bytetransformer/qkv-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/qkv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>为了后续的GEMM的计算，这一步将数据重新排布，排布之后的矩阵数据是 $head_{num} \times B\times S \times size_per_head$, 这个地方抛线程是如下：</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dim3</span> <span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">;</span>
<span class="n">grid</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">grid</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">;</span><span class="c1">//这个block的方式是按照输出矩阵的维度去安排的，也就是一个block去排布head_num_ *size_per_head这么多数据</span>
<span class="n">block</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">head_num_</span> <span class="o">*</span><span class="p">(</span><span class="n">size_per_head_</span> <span class="o">/</span> <span class="mi">2</span><span class="p">);</span>  <span class="c1">//这个地方除2是为了一次写出2个数据</span>
</code></pre></div></div> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//add_QKV_bias： 这个kernel主要理解好src_id和trt_id就可以了，也就是从QKV数据矩阵到转置之后的head_numXBXSXsize_per_head矩阵</span>
<span class="kt">int</span> <span class="n">batch_id</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">seq_id</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">head_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="n">half_size_per_head</span><span class="p">;</span> <span class="c1">// 这个就是head id</span>
<span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="n">half_size_per_head</span><span class="p">;</span><span class="c1">//这个相当于size_per_head的维度的id</span>
<span class="kt">int</span> <span class="n">src_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="c1">// batch_id * S * 3H + seq_id * 3H + threadIdx.x 注意threadIdx.x是head_num*size_per_head这个方向的id，看上图的数据排布</span>
<span class="kt">int</span> <span class="n">trt_id</span> <span class="o">=</span> <span class="p">((</span><span class="n">head_id</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="n">batch_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">seq_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">half_size_per_head</span> <span class="o">+</span> <span class="n">id</span><span class="p">;</span>
<span class="c1">// 目标数据的排布是head_numXBXSXsize_per_head</span>
</code></pre></div></div> <p>这个地方只需要注意一次写2个数据，所以抛线程除了2；这个<code class="language-plaintext highlighter-rouge">add_QKV_bias_padding</code>的kernel会把去掉的填充数据还原。</p> <h4 id="1-gemm-q-times-kt">1. GEMM $Q \times K^T$</h4> <p>这一步很简单，直接调用了cublas的Batch GEMM, <code class="language-plaintext highlighter-rouge">cublas_Gemm_Strided_Batched</code>这个函数，注意这个函数是列主序，而且计算的是$Q \times K^T$，所以要去注意函数传入的参数。</p> <h4 id="2-softmax">2. softmax</h4> <p>这个步骤涉及到三个kernel：</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 优化分支，如果seq_len是偶数并且是half才进入，要求seq_len是偶数的原因是因为一次要处理2个half数据</span>
<span class="k">if</span> <span class="p">((</span><span class="n">seq_len</span> <span class="o">&amp;</span> <span class="mh">0x1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">OpType</span> <span class="o">==</span> <span class="n">OperationType</span><span class="o">::</span><span class="n">HALF</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">1024</span><span class="p">)</span> <span class="p">{</span>
     <span class="c1">//SOFTMAX_HALF2_REG(xxx) //这个kernel是要利用寄存器来计算和存储中间结果</span>
     <span class="c1">//......</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">shmem_size</span> <span class="o">&gt;</span> <span class="mi">48</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">cudaFuncSetAttribute</span><span class="p">(</span><span class="n">softmax_kernel_warp_half2</span><span class="o">&lt;</span><span class="n">half2</span><span class="o">&gt;</span><span class="p">,</span>
                             <span class="n">cudaFuncAttributeMaxDynamicSharedMemorySize</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">);</span>
      <span class="c1">//这个kernel是要利用shared memory来计算和存储中间结果，其他思路是一样的</span>
      <span class="c1">//但是这个分支进不了，因为bytetransformer最大的seq len就是1024，可以对照的学习一下</span>
      <span class="n">softmax_kernel_warp_half2</span><span class="o">&lt;</span><span class="n">half2</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shmem_size</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
          <span class="p">(</span><span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">qk_buf</span><span class="p">,</span> <span class="p">(</span><span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">atten_bias</span><span class="p">,</span> <span class="p">(</span><span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">atten_mask</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">head_num</span><span class="p">,</span>
          <span class="n">seq_len</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">shmem_size</span> <span class="o">&gt;</span> <span class="mi">48</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
      <span class="n">cudaFuncSetAttribute</span><span class="p">(</span><span class="n">softmax_kernel_warp</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">cudaFuncAttributeMaxDynamicSharedMemorySize</span><span class="p">,</span>
                           <span class="mi">64</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">);</span>
    <span class="c1">//这个分支是如果seq_len是奇数或者数据是float的时候进入，基本思路和SOFTMAX_HALF2_REG相差不多，对照一起看</span>
    <span class="n">softmax_kernel_warp</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shmem_size</span><span class="p">,</span> <span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">qk_buf</span><span class="p">,</span> <span class="n">atten_bias</span><span class="p">,</span> <span class="n">atten_mask</span><span class="p">,</span>
                                                                <span class="n">batch_size</span><span class="p">,</span> <span class="n">head_num</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">);</span>
  <span class="p">}</span>
</code></pre></div></div> <p>这里我们主要看一下SOFTMAX_HALF2_REG这个kernel就可以了，其他的kernel思路是一样的。</p> <p>首先是线程抛出的逻辑：</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dim3</span> <span class="nf">grid</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">block</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">head_num</span><span class="p">);</span>
</code></pre></div></div> <p>也就是总共有batch_size * seq_len个block，每个block抛出head_num个warp（1个warp32个线程）。需要注意的是这个地方的输入是$Q \times K^T$之后的矩阵，尺寸是$head_num \times B\times S\times S$,也就是1个warp处理一行seq_len的数据。</p> <p>然后就是计算了(seq_len + 63) / 64，这个值代表了一行数据有多少个64，为什么是64？，因为1个warp是32个线程，1个线程处理2个half，所以是64，将seq_len行方向按照64分一下。</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//这里的REG_COUNT就是2*多少个64，为什么是乘2，因为一次处理2个half,需要存储这个两个half相应的最大值</span>
<span class="c1">//按照是否是64的倍数分开，只有传参true和false的区别，具体原因参考下图</span>
<span class="cp">#define SOFTMAX_HALF2_REG(REG_COUNT)                                                            \
  if (seq_len % 64 == 0)                                                                        \
    softmax_kernel_warp_half2_register&lt;half2, REG_COUNT, false&gt;                                 \
        &lt;&lt;&lt;grid, block, 0, stream&gt;&gt;&gt;((half2 *)qk_buf, (half2 *)atten_bias, (half2 *)atten_mask, \
                                     batch_size, head_num, seq_len);                            \
  else                                                                                          \
    softmax_kernel_warp_half2_register&lt;half2, REG_COUNT, true&gt;&lt;&lt;&lt;grid, block, 0, stream&gt;&gt;&gt;(     \
        (half2 *)qk_buf, (half2 *)atten_bias, (half2 *)atten_mask, batch_size, head_num, seq_len)
</span></code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/softmax-480.webp 480w,/assets/img/AI/bytetransformer/softmax-800.webp 800w,/assets/img/AI/bytetransformer/softmax-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/softmax.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">count</span><span class="p">,</span> <span class="k">const</span> <span class="kt">bool</span> <span class="n">need_padding</span><span class="p">&gt;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">softmax_kernel_warp_half2_register</span><span class="p">(</span><span class="n">half2</span> <span class="o">*</span><span class="n">qk_buf</span><span class="p">,</span> <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">atten_bias</span><span class="p">,</span>
                                                   <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">atten_mask</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span>
                                                   <span class="k">const</span> <span class="kt">int</span> <span class="n">head_num</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// dim3 grid(batch_size * seq_len)</span>
  <span class="c1">// block(32, head_num);</span>
  <span class="kt">int</span> <span class="n">word_id</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="c1">// 这个就grid的维度</span>
  <span class="kt">int</span> <span class="n">batch_id</span> <span class="o">=</span> <span class="n">word_id</span> <span class="o">/</span> <span class="n">seq_len</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">seq_id</span> <span class="o">=</span> <span class="n">word_id</span> <span class="o">%</span> <span class="n">seq_len</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">warp_tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="c1">//这个是线程id</span>
  <span class="kt">int</span> <span class="n">head_id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="c1">//head维度</span>
  <span class="kt">int</span> <span class="n">half2_seq_len</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
  <span class="c1">//索引矩阵数据 维度是head_numXBXSXS</span>
  <span class="kt">int</span> <span class="n">qk_offset</span> <span class="o">=</span> <span class="p">((</span><span class="n">head_id</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="n">batch_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">seq_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">half2_seq_len</span><span class="p">;</span>

  <span class="c1">//这个是上图中的存储最大值的数据 注意这个是寄存器数据，每个线程都有</span>
  <span class="kt">float</span> <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">count</span><span class="p">];</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">need_padding</span><span class="p">)</span> <span class="c1">// true和false的区别是 是否是64的倍数</span>
    <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">count</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10000.0</span><span class="n">f</span><span class="p">,</span> <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10000.0</span><span class="n">f</span><span class="p">;</span>

  <span class="kt">float</span> <span class="n">max_val</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e20</span><span class="n">f</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">col_id</span> <span class="o">=</span> <span class="n">warp_tid</span> <span class="o">+</span> <span class="n">warpSize</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">need_padding</span> <span class="o">&amp;&amp;</span> <span class="n">col_id</span> <span class="o">&gt;=</span> <span class="n">half2_seq_len</span><span class="p">)</span>
      <span class="k">break</span><span class="p">;</span>

    <span class="n">half2</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">qk_buf</span><span class="p">[</span><span class="n">qk_offset</span> <span class="o">+</span> <span class="n">col_id</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">atten_bias</span><span class="p">)</span>
      <span class="n">qk</span> <span class="o">=</span>
          <span class="n">__hadd2</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">atten_bias</span><span class="p">[((</span><span class="n">head_id</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">seq_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">half2_seq_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">col_id</span><span class="p">]));</span>
    <span class="n">half2</span> <span class="n">mask_val</span> <span class="o">=</span> <span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">atten_mask</span><span class="p">[((</span><span class="n">batch_id</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">seq_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">half2_seq_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">col_id</span><span class="p">]);</span>
    <span class="kt">float</span> <span class="n">mask_val_x</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="n">f</span> <span class="o">-</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">mask_val</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span><span class="n">f</span><span class="p">,</span>
          <span class="n">mask_val_y</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="n">f</span> <span class="o">-</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">mask_val</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">10000.0</span><span class="n">f</span><span class="p">;</span>
    <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">qk</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">mask_val_x</span><span class="p">,</span> <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">qk</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">mask_val_y</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="c1">//到这里 每个线程都有s_qk_buf 也就是每个线程都计算了一行的一部分数据</span>
  <span class="c1">//然后1个warp中的数据计算完 就用warpsum去计算这一行的所有数据的最大值</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">fmax</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
  <span class="n">max_val</span> <span class="o">=</span> <span class="n">warpReduceMax</span><span class="p">(</span><span class="n">max_val</span><span class="p">);</span>
  <span class="c1">//这里是计算指数值</span>
  <span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">);</span>
    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="p">}</span>
  <span class="c1">//同理 这个是个warpsum 求一行的指数和</span>
  <span class="n">exp_sum</span> <span class="o">=</span> <span class="n">warpReduceSum</span><span class="p">(</span><span class="n">exp_sum</span><span class="p">);</span>
  <span class="c1">//最后计算完然后写出数据</span>
  <span class="n">exp_sum</span> <span class="o">=</span> <span class="n">__fdividef</span><span class="p">(</span><span class="mf">1.0</span><span class="n">f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">count</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">col_id</span> <span class="o">=</span> <span class="n">warp_tid</span> <span class="o">+</span> <span class="n">warpSize</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">need_padding</span> <span class="o">&amp;&amp;</span> <span class="n">col_id</span> <span class="o">&gt;=</span> <span class="n">half2_seq_len</span><span class="p">)</span>
      <span class="k">return</span><span class="p">;</span>
    <span class="n">qk_buf</span><span class="p">[</span><span class="n">qk_offset</span> <span class="o">+</span> <span class="n">col_id</span><span class="p">]</span> <span class="o">=</span>
        <span class="n">__halves2half2</span><span class="p">((</span><span class="n">half</span><span class="p">)(</span><span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">exp_sum</span><span class="p">),</span> <span class="p">(</span><span class="n">half</span><span class="p">)(</span><span class="n">s_qk_buf</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">exp_sum</span><span class="p">));</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>关于warpReduceSum可以参考:</p> <p>https://zhuanlan.zhihu.com/p/572820783</p> <p>https://zhuanlan.zhihu.com/p/572901115</p> <h4 id="3-gemm-m_softmax-times-v">3. GEMM $M_{softmax} \times V$</h4> <p>这一步也很简单，也是直接调用了cublas的Batch GEMM, <code class="language-plaintext highlighter-rouge">cublas_Gemm_Strided_Batched</code>这个函数，要去注意函数传入的参数。</p> <h4 id="4-transpose">4. transpose</h4> <p>做完Attention之后，需要把数据重新变换回来，由 $head_{num} \times B\times S \times size_per_head$还原回$B\times S\times H$,以便后续计算。这个就是第0步的逆过程，可以结合着一起看，需要注意如果是<code class="language-plaintext highlighter-rouge">is_remove_padding_</code>为true，会再次将pad的数据去掉。</p> <h3 id="2-fused-kernel源码分析">2. fused kernel源码分析</h3> <p>进入到fused kernel这个分支之后，可以看到按照seq len是否小于等于80做了特化，如果小于等于80，那么执行<code class="language-plaintext highlighter-rouge">fused_infer</code>或者<code class="language-plaintext highlighter-rouge">fused_rm_infer</code>kernel，如果是在80到352之间那么执行<code class="language-plaintext highlighter-rouge">fused_long_infer</code>或者<code class="language-plaintext highlighter-rouge">fused_long_rm_infer</code>kernel，至于80和352这两个数字是怎么来的，应该是根据不同卡上的shared memory的大小得到的。</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">if</span> <span class="p">(</span><span class="n">use_fused_attention_</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">infer_param</span><span class="p">.</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">80</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">is_remove_padding_</span><span class="p">)</span>
          <span class="n">fused_rm_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
        <span class="k">else</span>
          <span class="n">fused_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">is_remove_padding_</span><span class="p">)</span>
          <span class="n">fused_long_rm_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
        <span class="k">else</span>
          <span class="n">fused_long_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span>
      <span class="nf">nofused_infer</span><span class="p">(</span><span class="n">infer_param</span><span class="p">);</span>
</code></pre></div></div> <h4 id="1-fused_infer源码">1. fused_infer源码</h4> <p>这个kernel执行的思想是， 将self attention这个步骤的几个kernel融合，减少IO带来的时间消耗。由于seq len小于等于80，占用的shared memory 较小，计算的时候将Q和K的矩阵全部加载的shared memory，然后开始计算Q和K的乘法，矩阵乘用到了WMMA，也就是Tensor Core。乘法做完之后在去做softmax，之后再去做和V的矩阵乘法，同样也是将V全部load到shared memory，最后写出结果。也就是说这个kernel需要同步四次。在做矩阵乘法的时候用到了WMMA，分块的小矩阵设置成16，也就是将矩阵分成16x16的块就做wmma乘法，因此在抛线程的时候除了16，也就是按照16x16的块去分。</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="n">Attention</span><span class="o">&lt;</span><span class="n">OpType</span><span class="o">&gt;::</span><span class="n">fused_infer</span><span class="p">(</span><span class="n">AttentionInferParam</span> <span class="n">infer_param</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">DataType_</span> <span class="o">*</span><span class="n">atten_mask</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">atten_mask</span><span class="p">;</span>
  <span class="n">DataType_</span> <span class="o">*</span><span class="n">attention_output</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">attention_output</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">batch_size</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">seq_len</span><span class="p">;</span>
  <span class="c1">//抛线程是按照head_numXBXSXsize_per_head这个方向去抛的</span>
  <span class="c1">//一共是head_numXB个block，每个block处理SXsize_per_head这个矩阵</span>
  <span class="n">dim3</span> <span class="n">grid</span><span class="p">(</span><span class="n">head_num_</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">),</span> <span class="n">block</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">OpType</span> <span class="o">==</span> <span class="n">OperationType</span><span class="o">::</span><span class="n">HALF</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">qkv_ptr</span> <span class="o">=</span> <span class="p">(</span><span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">infer_param</span><span class="p">.</span><span class="n">qkv</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">qkv_bias_ptr</span> <span class="o">=</span> <span class="p">(</span><span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">param_</span><span class="p">.</span><span class="n">attr_bias_QKV</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="n">f</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">size_per_head_</span> <span class="o">*</span> <span class="mf">1.0</span><span class="n">f</span><span class="p">))</span> <span class="o">/</span> <span class="n">param_</span><span class="p">.</span><span class="n">tao</span><span class="p">;</span>
    <span class="c1">// 1个warp计算1个16x16的矩阵，因为QK的乘完之后的结果是SxS,所以才有了 max(((seq_len + 15) / 16), size_per_head_ / 16)</span>
    <span class="c1">// 和V相乘的结果是Sxsize_per_head</span>
    <span class="n">block</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="p">((</span><span class="n">seq_len</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="n">max</span><span class="p">(((</span><span class="n">seq_len</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span> <span class="o">/</span> <span class="mi">16</span><span class="p">),</span> <span class="n">size_per_head_</span> <span class="o">/</span> <span class="mi">16</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">size_per_head_</span> <span class="o">==</span> <span class="mi">64</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">//seq_len 按照16 pad，因为分块矩阵的大小是16</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">WMMA_ATTENTION</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>
      <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">WMMA_ATTENTION</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>
      <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">48</span><span class="p">)</span>
        <span class="n">WMMA_ATTENTION</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>
      <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">WMMA_ATTENTION</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>
      <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">80</span><span class="p">)</span>
        <span class="n">WMMA_ATTENTION</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="mi">64</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">size_per_head_</span> <span class="o">==</span> <span class="mi">16</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">//这个分支走不到</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="mi">48</span><span class="p">)</span>
        <span class="n">WMMA_ATTENTION_16</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">16</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>需要注意的是attention的输入的矩阵尺寸是$B \times S \times H$, 但是矩阵计算是按照$head_num \times B \times S \times size_per_head$计算的，线程也是按照这个尺寸去抛的。</p> <p>代码第一部分：</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">const</span> <span class="kt">int</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">size_per_head</span><span class="p">&gt;</span>
<span class="n">__global__</span>
    <span class="kt">void</span>
    <span class="nf">wmma_attention_kernel</span><span class="p">(</span><span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">qkv</span><span class="p">,</span> <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">qkv_bias</span><span class="p">,</span> <span class="k">const</span> <span class="n">__half</span> <span class="o">*</span><span class="n">attention_mask</span><span class="p">,</span>
                          <span class="n">__half</span> <span class="o">*</span><span class="n">attention_output</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="n">scale</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#if __CUDA_ARCH__ &gt;= 700
</span>  <span class="k">using</span> <span class="k">namespace</span> <span class="n">nvcuda</span><span class="p">;</span>
  <span class="c1">// 分配shared memory，加上SKEW_HALF是为了防止bank conflict</span>
  <span class="c1">// 关于bank conflict 参考: https://on-demand.gputechconf.com/gtc/2018/presentation/s81006-volta-architecture-and-performance-optimization.pdf</span>
  <span class="c1">// 但这里处理了half的数据且size_per_head是64 也就是一行放了128个byte，正好填满整个bank，所以对s_kv</span>
  <span class="c1">// 和s_query来说并不存在bank conflict</span>
  <span class="n">__shared__</span> <span class="n">__half</span> <span class="n">s_kv</span><span class="p">[</span><span class="n">max_seq_len</span><span class="p">][</span><span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">];</span>
  <span class="n">__shared__</span> <span class="n">__half</span> <span class="n">s_query</span><span class="p">[</span><span class="n">max_seq_len</span><span class="p">][</span><span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">];</span>
  <span class="n">__shared__</span> <span class="n">__half</span> <span class="n">s_logits</span><span class="p">[</span><span class="n">max_seq_len</span><span class="p">][</span><span class="n">max_seq_len</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">];</span>

  <span class="k">const</span> <span class="kt">int</span> <span class="n">warpNums</span> <span class="o">=</span> <span class="p">(</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">5</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">warpId</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="mi">5</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_tid</span> <span class="o">=</span> <span class="n">getLaneId</span><span class="p">();</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">half_hidden_dim</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">size_per_head</span> <span class="o">/</span> <span class="mi">2</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">thread_offset</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">size_per_head</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">warp_tid</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">batch_seq_offset</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">seq_len</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">from_size</span> <span class="o">=</span> <span class="n">max_seq_len</span> <span class="o">/</span> <span class="mi">16</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">to_size</span> <span class="o">=</span> <span class="n">max_seq_len</span> <span class="o">/</span> <span class="mi">16</span><span class="p">;</span>

  <span class="c1">// loading Query &amp; Key</span>
  <span class="c1">// 将QK全部load进来，并且加上之前步骤的bias</span>
  <span class="c1">// 注意这个输入矩阵的尺寸是BS3H 因为是经过了GEMM了之后的矩阵，QKV在一起</span>
  <span class="c1">// 即:(B,S,H) X (H,3H) = (B,S,3H)</span>
  <span class="n">half2</span> <span class="n">q_bias</span> <span class="o">=</span> <span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qkv_bias</span><span class="p">[</span><span class="n">thread_offset</span><span class="p">]);</span>
  <span class="n">half2</span> <span class="n">k_bias</span> <span class="o">=</span> <span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qkv_bias</span><span class="p">[</span><span class="n">thread_offset</span> <span class="o">+</span> <span class="n">half_hidden_dim</span><span class="p">]);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">seq_id</span> <span class="o">=</span> <span class="n">warpId</span><span class="p">;</span> <span class="n">seq_id</span> <span class="o">&lt;</span> <span class="n">seq_len</span><span class="p">;</span> <span class="n">seq_id</span> <span class="o">+=</span> <span class="n">warpNums</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_seq_offset</span> <span class="o">+</span> <span class="n">seq_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">half_hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">thread_offset</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">seq_id</span> <span class="o">*</span> <span class="p">(</span><span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">warp_tid</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">);</span>
    <span class="o">*</span><span class="p">(</span><span class="n">__half2</span> <span class="o">*</span><span class="p">)(</span><span class="o">*</span><span class="n">s_query</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">=</span> <span class="n">__hadd2</span><span class="p">(</span><span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qkv</span><span class="p">[</span><span class="n">pos</span><span class="p">]),</span> <span class="n">q_bias</span><span class="p">);</span>
    <span class="o">*</span><span class="p">(</span><span class="n">__half2</span> <span class="o">*</span><span class="p">)(</span><span class="o">*</span><span class="n">s_kv</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">=</span> <span class="n">__hadd2</span><span class="p">(</span><span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qkv</span><span class="p">[</span><span class="n">pos</span> <span class="o">+</span> <span class="n">half_hidden_dim</span><span class="p">]),</span> <span class="n">k_bias</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">__syncthreads</span><span class="p">();</span><span class="c1">//这个地方等待所有的数据load完成</span>
</code></pre></div></div> <p>之后一部分是进行$Q \times K^T$的乘法:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/kernel_QK-480.webp 480w,/assets/img/AI/bytetransformer/kernel_QK-800.webp 800w,/assets/img/AI/bytetransformer/kernel_QK-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/kernel_QK.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// const int from_size = max_seq_len / 16;</span>
 <span class="c1">// const int to_size = max_seq_len / 16;</span>
<span class="c1">// 这个地方QK的乘法结果尺寸是seq_len X seq_len</span>
<span class="c1">// 按照结果的尺寸进行计算，每次计算是16X16的块，所以总共的warp数是 max_seq_len / 16 X max_seq_len / 16</span>
<span class="k">if</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">&lt;</span> <span class="n">from_size</span> <span class="o">*</span> <span class="n">to_size</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">Q_mat</span><span class="p">;</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span> <span class="n">K_mat</span><span class="p">;</span><span class="c1">// 注意是乘K的转置，所以是列主序</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="o">&gt;</span> <span class="n">QK_mat</span><span class="p">;</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">QK_mat</span><span class="p">,</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_from_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">/</span> <span class="n">to_size</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_to_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">%</span> <span class="n">to_size</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">;</span>

<span class="cp">#pragma unroll
</span>    <span class="c1">//这个地方为什么是4，因为size_per_head是64,64按照16分正好是分4部分</span>
    <span class="c1">//这个是矩阵乘法的内循环，只不过是按照块的乘法</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">Q_mat</span><span class="p">,</span> <span class="n">s_query</span><span class="p">[</span><span class="n">warp_from_offset</span><span class="p">]</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">WMMA_K</span><span class="p">,</span>
                             <span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">);</span>
      <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">K_mat</span><span class="p">,</span> <span class="n">s_kv</span><span class="p">[</span><span class="n">warp_to_offset</span><span class="p">]</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">);</span>
      <span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">QK_mat</span><span class="p">,</span> <span class="n">Q_mat</span><span class="p">,</span> <span class="n">K_mat</span><span class="p">,</span> <span class="n">QK_mat</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">s_logits</span><span class="p">[</span><span class="n">warp_from_offset</span><span class="p">]</span> <span class="o">+</span> <span class="n">warp_to_offset</span><span class="p">,</span> <span class="n">QK_mat</span><span class="p">,</span>
                            <span class="n">max_seq_len</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">__syncthreads</span><span class="p">();</span><span class="c1">//数据同步 等待QK计算完成</span>

</code></pre></div></div> <p>之后进行softmax的计算，这个步骤中融合了load V矩阵的过程，之前计算之后的k矩阵的shared memory已经用完不再使用，所以复用s_kv</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// softmax</span>
  <span class="n">half2</span> <span class="n">v_bias</span> <span class="o">=</span> <span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qkv_bias</span><span class="p">[</span><span class="n">thread_offset</span> <span class="o">+</span> <span class="n">half_hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]);</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">from_id</span> <span class="o">=</span> <span class="n">warpId</span><span class="p">;</span> <span class="n">from_id</span> <span class="o">&lt;</span> <span class="n">seq_len</span><span class="p">;</span> <span class="n">from_id</span> <span class="o">+=</span> <span class="n">warpNums</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_seq_len</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">/</span> <span class="mi">32</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">logits</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
    <span class="kt">int</span> <span class="n">to_id</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>

    <span class="kt">float</span> <span class="n">max_val</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e20</span><span class="n">f</span><span class="p">;</span>
<span class="cp">#pragma unroll
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">to_id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">warp_tid</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;&lt;</span> <span class="mi">5</span><span class="p">);</span>
      <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e20</span><span class="n">f</span><span class="p">;</span>

      <span class="k">if</span> <span class="p">(</span><span class="n">to_id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">seq_len</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">mask</span> <span class="o">=</span>
            <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">attention_mask</span><span class="p">[(</span><span class="n">batch_seq_offset</span> <span class="o">+</span> <span class="n">from_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">to_id</span><span class="p">[</span><span class="n">i</span><span class="p">]]);</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="n">f</span> <span class="o">-</span> <span class="n">mask</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">10000.0</span><span class="n">f</span><span class="p">);</span>
        <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)(</span><span class="n">s_logits</span><span class="p">[</span><span class="n">from_id</span><span class="p">][</span><span class="n">to_id</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">mask</span><span class="p">;</span>
      <span class="p">}</span>
      <span class="n">max_val</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">warpReduceMax</span><span class="p">(</span><span class="n">max_val</span><span class="p">);</span>

    <span class="kt">float</span> <span class="n">sum_val</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
<span class="cp">#pragma unroll
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">);</span>
      <span class="n">sum_val</span> <span class="o">+=</span> <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">sum_val</span> <span class="o">=</span> <span class="n">warpReduceSum</span><span class="p">(</span><span class="n">sum_val</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">;</span>

<span class="cp">#pragma unroll
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">to_id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_seq_len</span><span class="p">)</span>
        <span class="n">s_logits</span><span class="p">[</span><span class="n">from_id</span><span class="p">][</span><span class="n">to_id</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span><span class="n">__half</span><span class="p">)</span><span class="n">__fdividef</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sum_val</span><span class="p">);</span>

    <span class="c1">// loading Value</span>
    <span class="kt">int</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_seq_offset</span> <span class="o">+</span> <span class="n">from_id</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">half_hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">thread_offset</span><span class="p">;</span>
    <span class="p">((</span><span class="n">__half2</span> <span class="o">*</span><span class="p">)(</span><span class="n">s_kv</span><span class="p">[</span><span class="n">from_id</span><span class="p">]))[</span><span class="n">warp_tid</span><span class="p">]</span> <span class="o">=</span>
        <span class="n">__hadd2</span><span class="p">(</span><span class="n">__ldg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qkv</span><span class="p">[</span><span class="n">pos</span> <span class="o">+</span> <span class="n">half_hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">v_bias</span><span class="p">);</span>
  <span class="p">}</span>
</code></pre></div></div> <p>之后是一部清零的操作，因为输入的max_seq_len（16的倍数）和真实seq len之间有差别，也就是多出来的数据是随机的（上上步计算QK的时候也是，有些数据是随机的），为了计算V的矩阵乘法的正确性，将V的最后几行设置为0，计算的时候就可以抹去随机值带来的错误。</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// K dim clear 0</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">seq_id</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">warpId</span><span class="p">;</span> <span class="n">seq_id</span> <span class="o">&lt;</span> <span class="n">max_seq_len</span><span class="p">;</span> <span class="n">seq_id</span> <span class="o">+=</span> <span class="n">warpNums</span><span class="p">)</span>
    <span class="p">((</span><span class="kt">float</span> <span class="o">*</span><span class="p">)(</span><span class="n">s_kv</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]))[</span><span class="n">warp_tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
  <span class="n">__syncthreads</span><span class="p">();</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/kernel_zero-480.webp 480w,/assets/img/AI/bytetransformer/kernel_zero-800.webp 800w,/assets/img/AI/bytetransformer/kernel_zero-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/kernel_zero.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>最后一部分是V矩阵的乘法：</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//* V</span>
<span class="c1">// 这个地方为什么左移2，也就是乘4呢？ 和之前QK矩阵乘法类似，在做V矩阵乘法的时候，是SXS的矩阵和SXsize_per_head的矩阵相乘，结果尺寸还是SXsize_per_head</span>
<span class="c1">// 其中size_per_head=64，也就是将SXsize_per_head分成16X16的块，</span>
<span class="c1">// 所以应该是 (max_seq_len/16) * (size_per_head/16) = from_size*4 = from_size &lt;&lt; 2 个warp</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">from_size</span> <span class="o">&lt;&lt;</span> <span class="mi">2</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">Logits_mat</span><span class="p">;</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span> <span class="n">V_mat</span><span class="p">;</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span> <span class="n">WMMA_M</span><span class="p">,</span> <span class="n">WMMA_N</span><span class="p">,</span> <span class="n">WMMA_K</span><span class="p">,</span> <span class="n">half</span><span class="o">&gt;</span> <span class="n">QKV_mat</span><span class="p">;</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">QKV_mat</span><span class="p">,</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_from_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">&gt;&gt;</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">;</span><span class="c1">//warp id到矩阵数据的映射id</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_to_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">warpId</span> <span class="o">&amp;</span> <span class="mh">0x3</span><span class="p">)</span> <span class="o">*</span> <span class="n">WMMA_K</span><span class="p">;</span>

<span class="cp">#pragma unroll
</span>    <span class="c1">//这个地方和QK的乘法是类似,内循环变成了max_seq_len/16大小的块矩阵乘法</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">to_size</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">Logits_mat</span><span class="p">,</span> <span class="n">s_logits</span><span class="p">[</span><span class="n">warp_from_offset</span><span class="p">]</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">WMMA_K</span><span class="p">,</span>
                             <span class="n">max_seq_len</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">);</span>
      <span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">V_mat</span><span class="p">,</span> <span class="n">s_kv</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">WMMA_K</span><span class="p">]</span> <span class="o">+</span> <span class="n">warp_to_offset</span><span class="p">,</span> <span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">);</span>
      <span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">QKV_mat</span><span class="p">,</span> <span class="n">Logits_mat</span><span class="p">,</span> <span class="n">V_mat</span><span class="p">,</span> <span class="n">QKV_mat</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">s_query</span><span class="p">[</span><span class="n">warp_from_offset</span><span class="p">]</span> <span class="o">+</span> <span class="n">warp_to_offset</span><span class="p">,</span> <span class="n">QKV_mat</span><span class="p">,</span>
                            <span class="n">size_per_head</span> <span class="o">+</span> <span class="n">SKEW_HALF</span><span class="p">,</span> <span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="n">__syncthreads</span><span class="p">();</span><span class="c1">//同步</span>

<span class="c1">// 写出结果</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">from_id</span> <span class="o">=</span> <span class="n">warpId</span><span class="p">;</span> <span class="n">from_id</span> <span class="o">&lt;</span> <span class="n">seq_len</span><span class="p">;</span> <span class="n">from_id</span> <span class="o">+=</span> <span class="n">warpNums</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_seq_offset</span> <span class="o">+</span> <span class="n">from_id</span><span class="p">)</span> <span class="o">*</span> <span class="n">half_hidden_dim</span> <span class="o">+</span> <span class="n">thread_offset</span><span class="p">;</span>
    <span class="p">((</span><span class="n">__half2</span> <span class="o">*</span><span class="p">)(</span><span class="n">attention_output</span><span class="p">))[</span><span class="n">pos</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">__half2</span> <span class="o">*</span><span class="p">)(</span><span class="n">s_query</span><span class="p">[</span><span class="n">from_id</span><span class="p">]))[</span><span class="n">warp_tid</span><span class="p">];</span>
  <span class="p">}</span>
<span class="cp">#endif
</span><span class="p">}</span>
</code></pre></div></div> <h4 id="2-fused_long_infer源码">2. fused_long_infer源码</h4> <p>当seq len较大的时候，将QK全部load到shared memory就不合适了，毕竟shared memory是有限的。执行这个分支的seq len 是大于等于80 小于等于352，这些参数在不同的平台上可能不同，因为ByteTransformer主要运行在A100上，使用在别的平台的时候参数可能需要按照需要调整一下。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/AI/bytetransformer/fused_long-480.webp 480w,/assets/img/AI/bytetransformer/fused_long-800.webp 800w,/assets/img/AI/bytetransformer/fused_long-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/AI/bytetransformer/fused_long.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>这部分的代码的逻辑是，既然QK不能同时加载进shared memory，那就将Q拆分成块，每个小块加载进shared memory，但是K需要全部加载的，因为QK的乘法是需要全部的K的，之后在做softmax然后在和V相乘得到结果，其中将Q拆分成块之后的计算kernel，和fused_infer里面的kernel极其相似，只不过load Q的时候有差别。</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="n">OperationType</span> <span class="n">OpType</span><span class="p">&gt;</span>
<span class="kt">void</span> <span class="n">Attention</span><span class="o">&lt;</span><span class="n">OpType</span><span class="o">&gt;::</span><span class="n">fused_long_infer</span><span class="p">(</span><span class="n">AttentionInferParam</span> <span class="n">infer_param</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">const</span> <span class="n">DataType_</span> <span class="o">*</span><span class="n">atten_mask</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">atten_mask</span><span class="p">;</span>
  <span class="n">DataType_</span> <span class="o">*</span><span class="n">attention_output</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">attention_output</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">batch_size</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">seq_len</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">OpType</span> <span class="o">==</span> <span class="n">OperationType</span><span class="o">::</span><span class="n">HALF</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">qkv_ptr</span> <span class="o">=</span> <span class="p">(</span><span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">infer_param</span><span class="p">.</span><span class="n">qkv</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="n">qkv_bias_ptr</span> <span class="o">=</span> <span class="p">(</span><span class="k">const</span> <span class="n">half2</span> <span class="o">*</span><span class="p">)</span><span class="n">param_</span><span class="p">.</span><span class="n">attr_bias_QKV</span><span class="p">;</span>

    <span class="kt">float</span> <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="n">f</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">size_per_head_</span> <span class="o">*</span> <span class="mf">1.0</span><span class="n">f</span><span class="p">))</span> <span class="o">/</span> <span class="n">param_</span><span class="p">.</span><span class="n">tao</span><span class="p">;</span>

    <span class="n">dim3</span> <span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">shared_memory_size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">//这个地方其实之前说过，就是按照16去划分块</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">split_count</span> <span class="o">=</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span> <span class="o">/</span> <span class="mi">16</span><span class="p">;</span>
    <span class="k">switch</span> <span class="p">(</span><span class="n">split_count</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">//......</span>
        <span class="c1">//WMMA_ATTENTION_LONG</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="cp">#define WMMA_ATTENTION_LONG(SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN)                                    \
  shared_memory_size =                                                                            \
      ((SEQ_LEN + SPLIT_LEN) * (SIZE_PER_HEAD + SKEW_HALF) + SPLIT_LEN * (SEQ_LEN + SKEW_HALF)) * \
      2;                                                                                          \
  if (shared_memory_size &gt; 48 * 1024)                                                             \
    cudaFuncSetAttribute(wmma_attention_long_kernel&lt;SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN&gt;,           \
                         cudaFuncAttributeMaxDynamicSharedMemorySize, 64 * 1024);                 \
//因为将Q拆分了多块，所以在抛线程的时候，就多抛了一个维度，就是Q的分块的维度
</span><span class="n">grid</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">head_num_</span><span class="p">,</span> <span class="n">grid</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">SEQ_LEN</span> <span class="o">+</span> <span class="n">SPLIT_LEN</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">SPLIT_LEN</span><span class="p">,</span> <span class="n">grid</span><span class="p">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span>        \
<span class="c1">// block的逻辑和之前的一样，就是几个warp来计算Q的一小块</span>
  <span class="n">block</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="p">(</span><span class="n">SPLIT_LEN</span> <span class="o">/</span> <span class="mi">16</span> <span class="o">*</span> <span class="n">split_count</span><span class="p">);</span>                                                  \
  <span class="n">wmma_attention_long_kernel</span><span class="o">&lt;</span><span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">SIZE_PER_HEAD</span><span class="p">,</span> <span class="n">SPLIT_LEN</span><span class="o">&gt;</span>                                   \
      <span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shared_memory_size</span><span class="p">,</span> <span class="n">infer_param</span><span class="p">.</span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>                                  \
          <span class="n">qkv_ptr</span><span class="p">,</span> <span class="n">qkv_bias_ptr</span><span class="p">,</span> <span class="p">(</span><span class="n">__half</span> <span class="o">*</span><span class="p">)</span><span class="n">atten_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">__half</span> <span class="o">*</span><span class="p">)</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span>       \
          <span class="n">scale</span><span class="p">)</span>


<span class="c1">//这里shared memory的计算是这样的：</span>
<span class="c1">// __shared__ __half     s_kv  [max_seq_len][size_per_head + SKEW_HALF];</span>
<span class="c1">// __shared__ __half  s_query[split_seq_len][size_per_head + SKEW_HALF];</span>
<span class="c1">// __shared__ __half s_logits[split_seq_len][max_seq_len   + SKEW_HALF];</span>
<span class="c1">// 因为Qload的一块 所以大小是split_seq_lenXsize_per_head  K是全部load 所以是 max_seq_lenXsize_per_head</span>
<span class="c1">// s_logits是QK的结果所以是split_seq_lenXmax_seq_len</span>

</code></pre></div></div> <p>想明白了这些，在去看<code class="language-plaintext highlighter-rouge">wmma_attention_long_kernel</code>这个kernel就清楚很多了，对照着<code class="language-plaintext highlighter-rouge">wmma_attention_kernel</code>看就可以了，基本上是一样的。</p> <h2 id="5关于其它层的解释step-4--step-8">5.关于其它层的解释（step 4 ~ step 8）</h2> <p>剩下的层就是FFN和layernorm相关的kernel了，和GEMM相关的是用了cublas， layernorm相关的kernel也相对比较简单，唯一复杂的是<code class="language-plaintext highlighter-rouge">gemm_bias_gelu</code>这个kernel，是将GEMM和激活层融合到了一起，使用的是cutlass的自定义实现，这部分也是没有看，后续可以继续补充。</p> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Damons . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://github.com/DamonsJ/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>